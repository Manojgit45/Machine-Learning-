{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"5Wm7D1mio5Hz"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YFJ-5T9TpEtP"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TN3gNWKQpinY"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IS5YtKehpka3"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u6jWOMAoplum"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":141},"executionInfo":{"elapsed":539,"status":"error","timestamp":1732971482893,"user":{"displayName":"Manoj Kumar Meesala","userId":"13132702620588273420"},"user_tz":-330},"id":"3V1oEKoApv3N","outputId":"2d98c7cb-8e32-4898-92eb-6fabcdd0a421"},"outputs":[{"ename":"NameError","evalue":"name 'df' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-b666bf274d0a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"]}],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e4YF_cy9p2MK"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y0hvDh1Gp-W7"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aeU-nCm-qcsw"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wt2NVOIGq24s"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OWhBB1YirUMi"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WZG9G5fJsRPN"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OQiwwVN9sTTp"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9UBgPeg0sYop"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZScxDCPLsmhY"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WhiWXtddt3NZ"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ksQC1epjuFqW"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"15xgrpxDuKKd"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sXX9-x_huXY3"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eXoAYTkJvF9A"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f1XKNhTrwqNr"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"Q8fDw0LixmwF"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b0gLJQ8ww1LQ"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d9HchNIwxLqf"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B8Eqd8c5yOiN"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wxnH6kqzyd1J"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":152664,"status":"ok","timestamp":1732972497105,"user":{"displayName":"Manoj Kumar Meesala","userId":"13132702620588273420"},"user_tz":-330},"id":"FVYXznP8EzKH","outputId":"67c243c1-1695-4034-e2c8-7ccd2c772465"},"outputs":[{"name":"stdout","output_type":"stream","text":["                                                text  label\n","0  I grew up (b. 1965) watching and loving the Th...      0\n","1  When I put this movie in my DVD player, and sa...      0\n","2  Why do people who do not know what a particula...      0\n","3  Even though I have great interest in Biblical ...      0\n","4  Im a die hard Dads Army fan and nothing will e...      1\n","(40000, 2)\n","Index(['text', 'label'], dtype='object')\n","        text                                                               \n","       count unique                                                top freq\n","label                                                                      \n","0      20019  19815  This show comes up with interesting locations ...    3\n","1      19981  19908  Hilarious, clean, light-hearted, and quote-wor...    4\n","                                                text  label\n","0  I grew up (b. 1965) watching and loving the Th...      0\n","1  When I put this movie in my DVD player, and sa...      0\n","2  Why do people who do not know what a particula...      0\n","0        0\n","1        0\n","2        0\n","3        0\n","4        1\n","        ..\n","39995    1\n","39996    1\n","39997    0\n","39998    1\n","39999    1\n","Name: label, Length: 40000, dtype: int64\n"]},{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading collection 'all'\n","[nltk_data]    | \n","[nltk_data]    | Downloading package abc to /root/nltk_data...\n","[nltk_data]    |   Package abc is already up-to-date!\n","[nltk_data]    | Downloading package alpino to /root/nltk_data...\n","[nltk_data]    |   Package alpino is already up-to-date!\n","[nltk_data]    | Downloading package averaged_perceptron_tagger to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n","[nltk_data]    |       to-date!\n","[nltk_data]    | Downloading package averaged_perceptron_tagger_eng to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package averaged_perceptron_tagger_eng is already\n","[nltk_data]    |       up-to-date!\n","[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n","[nltk_data]    |       up-to-date!\n","[nltk_data]    | Downloading package averaged_perceptron_tagger_rus to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package averaged_perceptron_tagger_rus is already\n","[nltk_data]    |       up-to-date!\n","[nltk_data]    | Downloading package basque_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package basque_grammars is already up-to-date!\n","[nltk_data]    | Downloading package bcp47 to /root/nltk_data...\n","[nltk_data]    |   Package bcp47 is already up-to-date!\n","[nltk_data]    | Downloading package biocreative_ppi to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n","[nltk_data]    | Downloading package bllip_wsj_no_aux to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n","[nltk_data]    | Downloading package book_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package book_grammars is already up-to-date!\n","[nltk_data]    | Downloading package brown to /root/nltk_data...\n","[nltk_data]    |   Package brown is already up-to-date!\n","[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n","[nltk_data]    |   Package brown_tei is already up-to-date!\n","[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n","[nltk_data]    |   Package cess_cat is already up-to-date!\n","[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n","[nltk_data]    |   Package cess_esp is already up-to-date!\n","[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n","[nltk_data]    |   Package chat80 is already up-to-date!\n","[nltk_data]    | Downloading package city_database to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package city_database is already up-to-date!\n","[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n","[nltk_data]    |   Package cmudict is already up-to-date!\n","[nltk_data]    | Downloading package comparative_sentences to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package comparative_sentences is already up-to-\n","[nltk_data]    |       date!\n","[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n","[nltk_data]    |   Package comtrans is already up-to-date!\n","[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n","[nltk_data]    |   Package conll2000 is already up-to-date!\n","[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n","[nltk_data]    |   Package conll2002 is already up-to-date!\n","[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n","[nltk_data]    |   Package conll2007 is already up-to-date!\n","[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n","[nltk_data]    |   Package crubadan is already up-to-date!\n","[nltk_data]    | Downloading package dependency_treebank to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package dependency_treebank is already up-to-date!\n","[nltk_data]    | Downloading package dolch to /root/nltk_data...\n","[nltk_data]    |   Package dolch is already up-to-date!\n","[nltk_data]    | Downloading package europarl_raw to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package europarl_raw is already up-to-date!\n","[nltk_data]    | Downloading package extended_omw to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package extended_omw is already up-to-date!\n","[nltk_data]    | Downloading package floresta to /root/nltk_data...\n","[nltk_data]    |   Package floresta is already up-to-date!\n","[nltk_data]    | Downloading package framenet_v15 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package framenet_v15 is already up-to-date!\n","[nltk_data]    | Downloading package framenet_v17 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package framenet_v17 is already up-to-date!\n","[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n","[nltk_data]    |   Package gazetteers is already up-to-date!\n","[nltk_data]    | Downloading package genesis to /root/nltk_data...\n","[nltk_data]    |   Package genesis is already up-to-date!\n","[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n","[nltk_data]    |   Package gutenberg is already up-to-date!\n","[nltk_data]    | Downloading package ieer to /root/nltk_data...\n","[nltk_data]    |   Package ieer is already up-to-date!\n","[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n","[nltk_data]    |   Package inaugural is already up-to-date!\n","[nltk_data]    | Downloading package indian to /root/nltk_data...\n","[nltk_data]    |   Package indian is already up-to-date!\n","[nltk_data]    | Downloading package jeita to /root/nltk_data...\n","[nltk_data]    |   Package jeita is already up-to-date!\n","[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n","[nltk_data]    |   Package kimmo is already up-to-date!\n","[nltk_data]    | Downloading package knbc to /root/nltk_data...\n","[nltk_data]    |   Package knbc is already up-to-date!\n","[nltk_data]    | Downloading package large_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package large_grammars is already up-to-date!\n","[nltk_data]    | Downloading package lin_thesaurus to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n","[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n","[nltk_data]    |   Package mac_morpho is already up-to-date!\n","[nltk_data]    | Downloading package machado to /root/nltk_data...\n","[nltk_data]    |   Package machado is already up-to-date!\n","[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n","[nltk_data]    |   Package masc_tagged is already up-to-date!\n","[nltk_data]    | Downloading package maxent_ne_chunker to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n","[nltk_data]    | Downloading package maxent_ne_chunker_tab to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package maxent_ne_chunker_tab is already up-to-\n","[nltk_data]    |       date!\n","[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n","[nltk_data]    |       to-date!\n","[nltk_data]    | Downloading package maxent_treebank_pos_tagger_tab to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package maxent_treebank_pos_tagger_tab is already\n","[nltk_data]    |       up-to-date!\n","[nltk_data]    | Downloading package moses_sample to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package moses_sample is already up-to-date!\n","[nltk_data]    | Downloading package movie_reviews to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package movie_reviews is already up-to-date!\n","[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n","[nltk_data]    |   Package mte_teip5 is already up-to-date!\n","[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n","[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n","[nltk_data]    | Downloading package names to /root/nltk_data...\n","[nltk_data]    |   Package names is already up-to-date!\n","[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n","[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n","[nltk_data]    | Downloading package nonbreaking_prefixes to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n","[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n","[nltk_data]    |   Package nps_chat is already up-to-date!\n","[nltk_data]    | Downloading package omw to /root/nltk_data...\n","[nltk_data]    |   Package omw is already up-to-date!\n","[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]    |   Package omw-1.4 is already up-to-date!\n","[nltk_data]    | Downloading package opinion_lexicon to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n","[nltk_data]    | Downloading package panlex_swadesh to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n","[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n","[nltk_data]    |   Package paradigms is already up-to-date!\n","[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n","[nltk_data]    |   Package pe08 is already up-to-date!\n","[nltk_data]    | Downloading package perluniprops to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package perluniprops is already up-to-date!\n","[nltk_data]    | Downloading package pil to /root/nltk_data...\n","[nltk_data]    |   Package pil is already up-to-date!\n","[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n","[nltk_data]    |   Package pl196x is already up-to-date!\n","[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n","[nltk_data]    |   Package porter_test is already up-to-date!\n","[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n","[nltk_data]    |   Package ppattach is already up-to-date!\n","[nltk_data]    | Downloading package problem_reports to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package problem_reports is already up-to-date!\n","[nltk_data]    | Downloading package product_reviews_1 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n","[nltk_data]    | Downloading package product_reviews_2 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n","[nltk_data]    | Downloading package propbank to /root/nltk_data...\n","[nltk_data]    |   Package propbank is already up-to-date!\n","[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n","[nltk_data]    |   Package pros_cons is already up-to-date!\n","[nltk_data]    | Downloading package ptb to /root/nltk_data...\n","[nltk_data]    |   Package ptb is already up-to-date!\n","[nltk_data]    | Downloading package punkt to /root/nltk_data...\n","[nltk_data]    |   Package punkt is already up-to-date!\n","[nltk_data]    | Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]    |   Package punkt_tab is already up-to-date!\n","[nltk_data]    | Downloading package qc to /root/nltk_data...\n","[nltk_data]    |   Package qc is already up-to-date!\n","[nltk_data]    | Downloading package reuters to /root/nltk_data...\n","[nltk_data]    |   Package reuters is already up-to-date!\n","[nltk_data]    | Downloading package rslp to /root/nltk_data...\n","[nltk_data]    |   Package rslp is already up-to-date!\n","[nltk_data]    | Downloading package rte to /root/nltk_data...\n","[nltk_data]    |   Package rte is already up-to-date!\n","[nltk_data]    | Downloading package sample_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package sample_grammars is already up-to-date!\n","[nltk_data]    | Downloading package semcor to /root/nltk_data...\n","[nltk_data]    |   Package semcor is already up-to-date!\n","[nltk_data]    | Downloading package senseval to /root/nltk_data...\n","[nltk_data]    |   Package senseval is already up-to-date!\n","[nltk_data]    | Downloading package sentence_polarity to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package sentence_polarity is already up-to-date!\n","[nltk_data]    | Downloading package sentiwordnet to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package sentiwordnet is already up-to-date!\n","[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n","[nltk_data]    |   Package shakespeare is already up-to-date!\n","[nltk_data]    | Downloading package sinica_treebank to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package sinica_treebank is already up-to-date!\n","[nltk_data]    | Downloading package smultron to /root/nltk_data...\n","[nltk_data]    |   Package smultron is already up-to-date!\n","[nltk_data]    | Downloading package snowball_data to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package snowball_data is already up-to-date!\n","[nltk_data]    | Downloading package spanish_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package spanish_grammars is already up-to-date!\n","[nltk_data]    | Downloading package state_union to /root/nltk_data...\n","[nltk_data]    |   Package state_union is already up-to-date!\n","[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n","[nltk_data]    |   Package stopwords is already up-to-date!\n","[nltk_data]    | Downloading package subjectivity to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package subjectivity is already up-to-date!\n","[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n","[nltk_data]    |   Package swadesh is already up-to-date!\n","[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n","[nltk_data]    |   Package switchboard is already up-to-date!\n","[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n","[nltk_data]    |   Package tagsets is already up-to-date!\n","[nltk_data]    | Downloading package tagsets_json to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package tagsets_json is already up-to-date!\n","[nltk_data]    | Downloading package timit to /root/nltk_data...\n","[nltk_data]    |   Package timit is already up-to-date!\n","[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n","[nltk_data]    |   Package toolbox is already up-to-date!\n","[nltk_data]    | Downloading package treebank to /root/nltk_data...\n","[nltk_data]    |   Package treebank is already up-to-date!\n","[nltk_data]    | Downloading package twitter_samples to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package twitter_samples is already up-to-date!\n","[nltk_data]    | Downloading package udhr to /root/nltk_data...\n","[nltk_data]    |   Package udhr is already up-to-date!\n","[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n","[nltk_data]    |   Package udhr2 is already up-to-date!\n","[nltk_data]    | Downloading package unicode_samples to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package unicode_samples is already up-to-date!\n","[nltk_data]    | Downloading package universal_tagset to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package universal_tagset is already up-to-date!\n","[nltk_data]    | Downloading package universal_treebanks_v20 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n","[nltk_data]    |       date!\n","[nltk_data]    | Downloading package vader_lexicon to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package vader_lexicon is already up-to-date!\n","[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n","[nltk_data]    |   Package verbnet is already up-to-date!\n","[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n","[nltk_data]    |   Package verbnet3 is already up-to-date!\n","[nltk_data]    | Downloading package webtext to /root/nltk_data...\n","[nltk_data]    |   Package webtext is already up-to-date!\n","[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n","[nltk_data]    |   Package wmt15_eval is already up-to-date!\n","[nltk_data]    | Downloading package word2vec_sample to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package word2vec_sample is already up-to-date!\n","[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n","[nltk_data]    |   Package wordnet is already up-to-date!\n","[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n","[nltk_data]    |   Package wordnet2021 is already up-to-date!\n","[nltk_data]    | Downloading package wordnet2022 to /root/nltk_data...\n","[nltk_data]    |   Package wordnet2022 is already up-to-date!\n","[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n","[nltk_data]    |   Package wordnet31 is already up-to-date!\n","[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n","[nltk_data]    |   Package wordnet_ic is already up-to-date!\n","[nltk_data]    | Downloading package words to /root/nltk_data...\n","[nltk_data]    |   Package words is already up-to-date!\n","[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n","[nltk_data]    |   Package ycoe is already up-to-date!\n","[nltk_data]    | \n","[nltk_data]  Done downloading collection all\n"]},{"name":"stdout","output_type":"stream","text":["Tokenized message: 0        [i, grew, up, (, b, ., 1965, ), watching, and,...\n","1        [when, i, put, this, movie, in, my, dvd, playe...\n","2        [why, do, people, who, do, not, know, what, a,...\n","3        [even, though, i, have, great, interest, in, b...\n","4        [im, a, die, hard, dads, army, fan, and, nothi...\n","                               ...                        \n","39995    [``, western, union, '', is, something, of, a,...\n","39996    [this, movie, is, an, incredible, piece, of, w...\n","39997    [my, wife, and, i, watched, this, movie, becau...\n","39998    [when, i, first, watched, flatliners, ,, i, wa...\n","39999    [why, would, this, film, be, so, good, ,, but,...\n","Name: tokenized_message, Length: 40000, dtype: object\n","Lemmatized message: 0        [i, grew, up, (, b, ., 1965, ), watching, and,...\n","1        [when, i, put, this, movie, in, my, dvd, playe...\n","2        [why, do, people, who, do, not, know, what, a,...\n","3        [even, though, i, have, great, interest, in, b...\n","4        [im, a, die, hard, dad, army, fan, and, nothin...\n","                               ...                        \n","39995    [``, western, union, '', is, something, of, a,...\n","39996    [this, movie, is, an, incredible, piece, of, w...\n","39997    [my, wife, and, i, watched, this, movie, becau...\n","39998    [when, i, first, watched, flatliners, ,, i, wa...\n","39999    [why, would, this, film, be, so, good, ,, but,...\n","Name: lemmatized_message, Length: 40000, dtype: object\n"]}],"source":["\n","import pandas as pd\n","import numpy as np\n","import csv\n","import string\n","\n","#Data Loading\n","imdb=pd.read_csv('/content/imdb.csv')\n","imdb.columns = [\"label\",\"text\"]\n","print(imdb.head(5))\n","\n","\n","data_size = imdb.shape\n","\n","print(data_size)\n","\n","imdb_col_names = imdb.columns\n","\n","print(imdb_col_names)\n","print(imdb.groupby('label').describe())\n","print(imdb.head(3))\n","\n","\n","imdb_target=imdb['label']\n","\n","print(imdb_target)\n","\n","from nltk.tokenize import word_tokenize\n","import nltk\n","nltk.download('all')\n","\n","\n","def split_tokens(text):\n","\n","  message = text.lower()\n","\n","\n","  word_tokens = word_tokenize(message)\n","\n","  return word_tokens\n","\n","imdb['tokenized_message'] = imdb['text'].apply(split_tokens)\n","\n","from nltk.stem.wordnet import WordNetLemmatizer\n","\n","def split_into_lemmas(text):\n","\n","    lemma = []\n","\n","    lemmatizer = WordNetLemmatizer()\n","\n","    for word in text:\n","\n","        a=lemmatizer.lemmatize(word)\n","\n","        lemma.append(a)\n","\n","    return lemma\n","\n","\n","\n","imdb['lemmatized_message'] = imdb['tokenized_message'].apply(split_into_lemmas)\n","\n","\n","\n","print('Tokenized message:',imdb['tokenized_message'])\n","\n","print('Lemmatized message:',imdb['lemmatized_message'])\n","\n"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":131368,"status":"ok","timestamp":1732980176269,"user":{"displayName":"Manoj Kumar Meesala","userId":"13132702620588273420"},"user_tz":-330},"id":"UgMhLW-FM1Ae","outputId":"567d2325-57f3-47ad-f52d-9f58ebcc87f4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Preprocessed message: 0        grew ( b . 1965 ) watching loving thunderbird ...\n","1        put movie dvd player , sat coke chip , expecta...\n","2        people know particular time past wa like feel ...\n","3        even though great interest biblical movie , wa...\n","4        im die hard dad army fan nothing ever change ....\n","                               ...                        \n","39995    `` western union '' something forgotten classi...\n","39996    movie incredible piece work . explores every n...\n","39997    wife watched movie plan visit sicily stromboli...\n","39998    first watched flatliners , wa amazed . necessa...\n","39999    would film good , gross estimated $ 95,000,000...\n","Name: preprocessed_message, Length: 40000, dtype: object\n"]}],"source":["from nltk.corpus import stopwords\n","imdb.sample(frac=0.1, random_state=42)\n","\n","\n","\n","def stopword_removal(text):\n","\n","    stop_words = stopwords.words('english')\n","\n","    filtered_sentence = []\n","\n","    filtered_sentence = ' '.join([word for word in text if word not in stop_words])\n","\n","    return filtered_sentence\n","\n","\n","\n","imdb['preprocessed_message'] = imdb['lemmatized_message'].apply(stopword_removal)\n","\n","print('Preprocessed message:',imdb['preprocessed_message'])\n","\n","Training_data=pd.Series(list(imdb['preprocessed_message']))\n","\n","Training_label=pd.Series(list(imdb['label']))\n","\n","\n","from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n","\n","tf_vectorizer = CountVectorizer(ngram_range=(1,2),min_df=(1/len(Training_label)),max_df=0.7)\n","\n","Total_Dictionary_TDM = tf_vectorizer.fit(Training_data)\n","\n","message_data_TDM = Total_Dictionary_TDM.transform(Training_data)\n","\n","from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n","\n","tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,2),min_df=(1/len(Training_label)),max_df=0.7)\n","\n","Total_Dictionary_TFIDF = tfidf_vectorizer.fit(Training_data)\n","\n","message_data_TFIDF = Total_Dictionary_TFIDF.transform(Training_data)\n","\n","\n","from sklearn.model_selection import train_test_split#Splitting the data for training and testing\n","\n","train_data,test_data, train_label, test_label = train_test_split(message_data_TDM,Training_label,test_size=0.1)\n","\n","\n","\n"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":297},"id":"Hl22o7iNO3x5","executionInfo":{"status":"error","timestamp":1732986929895,"user_tz":-330,"elapsed":6743084,"user":{"displayName":"Manoj Kumar Meesala","userId":"13132702620588273420"}},"outputId":"0a872a62-2d17-4e42-c4d2-99530d5d1e74"},"outputs":[{"output_type":"stream","name":"stdout","text":["The shape of train  : (36000, 2527102)\n","The shape of test data : (36000, 2527102)\n","SVM Classifier :  0.8955\n","The shape of train data: (32000, 2527102)\n","The shape of test data: (8000, 2527102)\n"]},{"output_type":"error","ename":"TypeError","evalue":"SGDClassifier.__init__() got an unexpected keyword argument 'Shuffle'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-23-393502956898>\u001b[0m in \u001b[0;36m<cell line: 40>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The shape of test data:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_data_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mSGDClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'modified_huber'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mShuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: SGDClassifier.__init__() got an unexpected keyword argument 'Shuffle'"]}],"source":["seed=9\n","from sklearn.svm import SVC\n","from sklearn.metrics import accuracy_score\n","train_data_shape = train_data.shape\n","\n","test_data_shape = test_data.shape\n","print(\"The shape of train  :\",train_data_shape)\n","\n","print(\"The shape of test data :\",train_data_shape)\n","\n","classifier = SVC(kernel='linear', C=0.025, random_state=seed)\n","\n","classifier = classifier.fit(train_data,train_label)\n","\n","target = classifier.predict(test_data)\n","\n","score = accuracy_score(test_label,target)\n","\n","print('SVM Classifier : ',score)\n","\n","\n","with open('output.txt', 'w') as file:\n","    file.write(str((imdb['tokenized_message'][55],imdb['lemmatized_message'][55])))\n","\n","from sklearn.linear_model import SGDClassifier\n","from sklearn.metrics import accuracy_score\n","\n","train_data,test_data, train_label, test_label = train_test_split(message_data_TDM,Training_label,test_size=0.2)\n","\n","train_data_shape = train_data.shape\n","\n","test_data_shape = test_data.shape\n","\n","print(\"The shape of train data:\",train_data_shape)\n","\n","\n","\n","print(\"The shape of test data:\",test_data_shape)\n","\n","classifier =  SGDClassifier(loss = 'modified_huber', Shuffle=True, random_state=seed)\n","\n","classifier = classifier.fit(train_data,train_label)\n","\n","target= classifier.predict(test_data)\n","\n","score = accuracy_score(test_label,target)\n","\n","print('SGD classifier : ',score)\n","\n","with open('output1.txt', 'w') as file:\n","    file.write(str((imdb['preprocessed_message'][55])))"]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN4kAyBk5F9bCnLZrxK32Bv"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}